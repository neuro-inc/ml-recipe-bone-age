kind: live
title: ml-recipe-bone-age

defaults:
  preset: cpu-small
  life_span: 1d

volumes:
  data:
    remote: storage://onprem-poc/alexeynayden/data
    mount: /ml-recipe-bone-age/data
    local: data
  remote_dataset:
    remote: storage://onprem-poc/alexeynayden/dataset
    mount: /ml-recipe-bone-age/dataset
  dvc_cache:
    remote: storage://onprem-poc/alexeynayden/dvc_cache
    mount: /ml-recipe-bone-age/dvc_cache
  project:
    remote: storage://onprem-poc/alexeynayden
    mount: /ml-recipe-bone-age
    local: .
  postgress_storage:
    remote: disk:disk-9299764c-fcd5-42b0-adcb-4313f62ce6be
    mount: /var/lib/postgresql/data
  mlflow_artifacts:
    remote: storage://onprem-poc/alexeynayden/mlruns
    mount: /usr/local/share/mlruns
    local: /usr/local/share/mlruns
  label_studio:
    remote: storage://onprem-poc/alexeynayden/label-studio
    mount: /ml-recipe-bone-age/label-studio-project
    local: label_studio
  src:
    remote: storage://onprem-poc/alexeynayden/src
    mount: /ml-recipe-bone-age/src
    local: src
  config:
    remote: storage://onprem-poc/alexeynayden/config
    mount: /ml-recipe-bone-age/config
    local: config

images:
  train:
    ref: image:$[[ flow.project_id ]]:21.1.23
    dockerfile: $[[ flow.workspace ]]/docker/train.dockerfile
    context: $[[ flow.workspace ]]/
    build_preset: cpu-small
  seldon:
    ref: image:$[[ flow.project_id ]]/seldon:21.1.23
    dockerfile: $[[ flow.workspace ]]/seldon_deployment/seldon.Dockerfile
    context: $[[ flow.workspace ]]/
    build_preset: cpu-large
  label_studio:
    ref: image:$[[ flow.project_id ]]/label_studio:21.1.18
    dockerfile: $[[ flow.workspace ]]/label_studio/label_studio.Dockerfile
    context: $[[ flow.workspace ]]/label_studio

jobs:

  prepare_remote_dataset:
    image: neuromation/neuro-extras:20.12.15
    preset: cpu-small
    detach: False
    volumes:
      - ${{ volumes.remote_dataset.ref_rw }}
    params:
      dataset_path:
        default: http://data.neu.ro/bone-age-tiny.zip
        descr: URL of dataset to download
    bash: |
      DST=$(basename ${{ params.dataset_path }} .tar)
      if [ ! -e ${{ volumes.remote_dataset.mount }}/$DST ]; then
        neuro-extras data cp --extract --use-temp-dir ${{ params.dataset_path }} ${{ volumes.remote_dataset.mount }}/$DST;
      else
        echo "Dataset is already downloaded, skipping."
      fi

  extend_data: # todo: need to adapt
    image: $[[ images.train.ref ]]
    preset: cpu-small
    detach: False
    volumes:
      - ${{ volumes.remote_dataset.ref_ro }}
      - ${{ volumes.dvc_cache.ref_rw }}
      - secret:/yevheniisemendiak/ysemendiak-gh-rsa:/root/.ssh/id-rsa
    params:
      extend_dataset_by:
        default: "1"
        descr: |
          How many new images to add into current dataset.
      target_git_branch:
        default: "ys/datasets"
      base_git_ref:
        default: "ys/neuro-flow"
    bash: |
      echo "Downloading base dataset"
      echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
      ssh-keyscan github.com >> ~/.ssh/known_hosts
      git clone git@github.com:neuro-inc/ml-recipe-bone-age.git project
      cd project
      git checkout ${{ params.base_git_ref }}
      git checkout -b ${{ params.target_git_branch }} || git pull

      dvc remote add -d mounted_cache $[[ volumes.dvc_cache.mount ]]
      dvc pull || echo "DVC cache not found, will be created..."

      echo "Extending dataset"
      python label_studio/extend_dataset.py \
        --cur_dataset data/Images \
        --full_dataset ${{ volumes.remote_dataset.mount }}/images/Images/ \
        --nmber_of_imgs ${{ params.extend_dataset_by }}

      echo "Commiting changes"
      dvc commit data/Images.dvc -f || dvc add data/Images/
      dvc push
      git add data/Images.dvc
      git config user.email "${NEURO_JOB_ID}@${NEURO_JOB_CLUSTER}"
      git config user.name "${NEURO_JOB_OWNER}"
      git commit -m "add ${{ params.extend_dataset_by }} img to dataset."
      git push --force -u origin ${{ params.target_git_branch }}

  label_studio: # todo: fix if we are doing it
    image: $[[ images.label_studio.ref ]]
    name: $[[ flow.title ]]-label-studio
    http_port: 443
    http_auth: False
    life_span: 1d
    detach: False
    browse: True
    volumes:
      - ${{ volumes.label_studio.ref_rw }}
      - ${{ volumes.dvc_cache.ref_rw }}
      - secret:/yevheniisemendiak/ysemendiak-gh-rsa:/root/.ssh/id-rsa
    params:
      target_git_branch:
        default: "ys/datasets"
    bash: |
      echo "Downloading base dataset"
      echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
      ssh-keyscan github.com >> ~/.ssh/known_hosts
      git clone git@github.com:neuro-inc/ml-recipe-bone-age.git project && cd project
      git checkout --track origin/${{ params.target_git_branch }}

      dvc remote add -d mounted_cache $[[ volumes.dvc_cache.mount ]]
      dvc pull
      cd ..

      label-studio init \
        --label-config ./project/label_studio/LabelConfig.xml \
        --input-path ./project/data/Images \
        --input-format image-dir \
        --host ml-recipe-bone-age-label-studio--yevheniisemendiak.jobs.neuro-compute.org.neu.ro \
        --port 443 \
        --allow-serving-local-files \
        label-studio-project

      echo "Copying finished labels into local project"
      cp -rT ${{ volumes.label_studio.mount }}/completions ./label-studio-project/completions || echo "No finished labels found!"
      echo "Starting rsync daemon to sync labels changes back from local FS to storage"
      lsyncd -delay 1 -rsync ./label-studio-project/completions ${{ volumes.label_studio.mount }}/completions

      echo "Starting label-studio"
      completions_count=$(ls ./label-studio-project/completions | grep -v ^d | wc -l) || completions_count=0
      python project/label_studio/launch_ls.py \
        --project_root ./project \
        --ls_project_root ./label-studio-project -- \
        start --use-gevent --no-browser ./label-studio-project
      new_completions_count=$(ls ./label-studio-project/completions | grep -v ^d | wc -l)

      echo "Commiting new version of dataset."
      cd project
      dvc commit data/Images.dvc -f && dvc push
      git status && git add data/Images.dvc data/result.json
      git config user.email "${NEURO_JOB_ID}@${NEURO_JOB_CLUSTER}"
      git config user.name "${NEURO_JOB_OWNER}"
      git commit -m "annotate $(( new_completions_count - completions_count )) images."
      git push

  postgres:
    image: postgres:12.5
    name: $[[ flow.title ]]-postgres
    http_port: 5432
    http_auth: False
    life_span: 30d
    detach: True
    volumes:
      - ${{ volumes.postgress_storage.remote }}:/var/lib/postgresql/data:rw
    env:
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: ""
      PGDATA: /var/lib/postgresql/data/pgdata

  mlflow_server:
    image: neuromation/mlflow:1.11.0
    name: $[[ flow.title ]]-mlflow-server
    http_port: 5000
    http_auth: False
    life_span: 30d
    detach: True
    volumes:
      - ${{ volumes.mlflow_artifacts.ref_rw }}
    cmd: |
      server --host 0.0.0.0
        --backend-store-uri=postgresql://postgres:password@ml-recipe-bone-age-postgres--yevheniisemendiak.platform-jobs:5432
        --default-artifact-root=${{ volumes.mlflow_artifacts.mount }}

  train: # todo: Y.S. working on it
    image: $[[ images.train.ref ]]
    preset: gpu-k80-small-p
    detach: False
    life_span: 10d
    volumes:
      - $[[ volumes.dvc_cache.ref_ro ]]
      - $[[ volumes.mlflow_artifacts.ref_rw ]]
      - secret:/yevheniisemendiak/ysemendiak-gh-rsa:/root/.ssh/id-rsa
    env:
      EXPOSE_SSH: "yes"
      MLFLOW_TRACKING_URI: "https://ml-recipe-bone-age-postgres-mlflow-server--yevheniisemendiak.jobs.neuro-compute.org.neu.ro"
      PYTHONPATH: $[[ volumes.project.mount ]]
    params:
      target_git_branch:
        default: "ys/datasets"
    workdir: $[[ volumes.project.mount ]]
    bash: |
        echo "IdentityFile ~/.ssh/id-rsa" > ~/.ssh/config
        ssh-keyscan github.com >> ~/.ssh/known_hosts
        git clone git@github.com:neuro-inc/ml-recipe-bone-age.git $[[ volumes.project.mount ]]
        cd $[[ volumes.project.mount ]]
        git checkout --track origin/${{ params.target_git_branch }}

        dvc remote add -d mounted_cache $[[ volumes.dvc_cache.mount ]]
        dvc pull
        python -u $[[ volumes.src.mount ]]/train.py \
          --data_dir=$[[ volumes.data.mount ]]/data/train \
          --annotation_csv=$[[ volumes.data.mount ]]/data/train.csv \
          || sleep infinity

  deploy_inference_platform: # todo: Alexey is working on it
    image: $[[ images.seldon.ref ]]
    name: $[[ flow.title ]]-test-inference
    preset: gpu-small
    http_port: 5000
    http_auth: False
    life_span: 5h
    detach: True
    env:
      PYTHONPATH: /microservice/src
    params:
      run_id:
    volumes:
      - ${{ volumes.mlflow_artifacts.remote }}/0/${{ params.run_id }}/artifacts/model/data/:/storage/

  locust: # todo: after inference
    image: locustio/locust:1.4.1
    name: $[[ flow.title ]]-locust
    http_port: 8080
    http_auth: False
    life_span: 1d
    detach: True
    browse: True
    params:
      endpoint_url:
    volumes:
      - $[[ upload(volumes.src).ref_ro ]]
      - $[[ volumes.data.ref_ro ]]
    env:
      DATA_DIR: $[[ volumes.data.mount ]]/locust
      PYTHONPATH: $[[ volumes.src.mount ]]/..
    cmd: |
      -f $[[ volumes.src.mount ]]/locust.py --web-port 8080 -H $[[ params.endpoint_url ]]

  filebrowser:
    action: gh:neuro-actions/filebrowser@master
    args:
      volumes_project_remote: $[[ volumes.project.remote ]]

  webdav_dvc:
    action: gh:neuro-actions/webdav_server@master
    args:
      volume_remote: $[[ volumes.dvc_cache.remote ]]
      job_name: $[[ flow.title ]]-webdav-dvc
      http_auth: ""

  webdav_mlflow:
    action: gh:neuro-actions/webdav_server@master
    args:
      volume_remote: $[[ volumes.mlflow_artifacts.remote ]]
      job_name: $[[ flow.title ]]-webdav-mlflow
      http_auth: ""
